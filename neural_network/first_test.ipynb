{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89dd3be0-9d2f-4d20-b3ec-c0e52f657bab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob, os\n",
    "import random\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU') #To list the available amount of GPUs\n",
    "print(physical_devices)\n",
    "\n",
    "\n",
    "try:\n",
    "    for kgpu in range(len(physical_devices)):\n",
    "        tf.config.experimental.set_memory_growth(physical_devices[kgpu], True)  #Set if memory growth should be enabled for a PhysicalDevice\n",
    "except:\n",
    "    # Invalid device or cannot modify virtual devices once initialized.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f670d80f-ad7b-4ea5-832e-c5d91f52db7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# in/out variable lists for v1\n",
    "\n",
    "vars_mli = ['state_t','state_q0001','state_ps','pbuf_SOLIN', 'pbuf_LHFLX', 'pbuf_SHFLX']\n",
    "vars_mlo = ['ptend_t','ptend_q0001','cam_out_NETSW','cam_out_FLWDS','cam_out_PRECSC','cam_out_PRECC','cam_out_SOLS','cam_out_SOLL','cam_out_SOLSD','cam_out_SOLLD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75b4d1dc-8e4d-46ff-af30-18a3ad61db2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#To noralize (with precomputed normalization factors)\n",
    "\n",
    "path = '/gpfswork/rech/psl/upu87pm/ClimSim'\n",
    "\n",
    "mli_mean = xr.open_dataset(path + '/preprocessing/normalizations/inputs/input_max.nc')\n",
    "mli_min = xr.open_dataset(path + '/preprocessing/normalizations/inputs/input_min.nc')\n",
    "mli_max = xr.open_dataset(path + '/preprocessing/normalizations/inputs/input_max.nc')\n",
    "mlo_scale = xr.open_dataset(path + '/preprocessing/normalizations/outputs/output_scale.nc')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "223b2b84-7fea-434a-b369-8f692ce8edea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#function defined in climsim_utils.data_utils, I should be able to import it \n",
    "\n",
    "\n",
    "def load_nc_dir_with_generator(filelist:list):\n",
    "    '''\n",
    "        This function works as a dataloader when training the emulator with raw netCDF files.\n",
    "        This can be used as a dataloader during training or it can be used to create entire datasets. !!\n",
    "        When used as a dataloader for training, I/O can slow down training considerably.\n",
    "        This function also normalizes the data.\n",
    "        mli corresponds to input\n",
    "        mlo corresponds to target\n",
    "        '''\n",
    "    def gen():\n",
    "        for file in filelist:\n",
    "            \n",
    "            # read mli\n",
    "            ds = xr.open_dataset(file, engine='netcdf4')\n",
    "            ds = ds[vars_mli]                             \n",
    "            \n",
    "            # read mlo\n",
    "            dso = xr.open_dataset(file.replace('.mli.','.mlo.'), engine='netcdf4') \n",
    "            \n",
    "            # make mlo variales: ptend_t and ptend_q0001\n",
    "            dso['ptend_t'] = (dso['state_t'] - ds['state_t'])/1200 # T tendency [K/s]\n",
    "            dso['ptend_q0001'] = (dso['state_q0001'] - ds['state_q0001'])/1200 # Q tendency [kg/kg/s]\n",
    "            dso = dso[vars_mlo]\n",
    "            \n",
    "            # normalizatoin, scaling\n",
    "            ds = (ds-mli_mean)/(mli_max-mli_min)\n",
    "          \n",
    "            dso = dso*mlo_scale\n",
    "\n",
    "            # stack\n",
    "            #ds = ds.stack({'batch':{'sample','ncol'}})\n",
    "            ds = ds.stack({'batch':{'ncol'}})\n",
    "            ds = ds.to_stacked_array(\"mlvar\", sample_dims=[\"batch\"], name='mli')\n",
    "            #dso = dso.stack({'batch':{'sample','ncol'}})\n",
    "            dso = dso.stack({'batch':{'ncol'}})\n",
    "            dso = dso.to_stacked_array(\"mlvar\", sample_dims=[\"batch\"], name='mlo')\n",
    "            \n",
    "            yield (ds.values, dso.values)\n",
    "\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        output_types=(tf.float64, tf.float64),\n",
    "        output_shapes=((None,124),(None,128))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0e68706-0cb6-4df5-b261-d2af291e8c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Total # of input files: 684\n",
      "[TRAIN] Total # of columns (nfiles * ncols): 262656\n",
      "[VAL] Total # of input files: 195\n",
      "[VAL] Total # of columns (nfiles * ncols): 74880\n"
     ]
    }
   ],
   "source": [
    "path_data = '/gpfsscratch/rech/yvd/upu87pm/lr'\n",
    "\n",
    "shuffle_buffer=12 \n",
    "\n",
    "# for training\n",
    "\n",
    "# # First 5 days of each month for the first 6 years\n",
    "# f_mli1 = glob.glob('/pscratch/sd/s/sungduk/hugging/E3SM-MMF_ne4/train/*/E3SM-MMF.mli.000[123456]-*-0[12345]-*.nc')\n",
    "# f_mli2 = glob.glob('/pscratch/sd/s/sungduk/hugging/E3SM-MMF_ne4/train/*/E3SM-MMF.mli.0007-01-0[12345]-*.nc')\n",
    "# f_mli = [*f_mli1, *f_mli2]\n",
    "\n",
    "\n",
    "# every 2th sample \n",
    "f_mli1 = glob.glob(path_data +'/0001/E3SM-MMF.mli.000[1]-10-0[123456789]-*.nc') #first year 10 first days of october for training (list of filename)\n",
    "f_mli2 = glob.glob(path_data +'/0001/E3SM-MMF.mli.0001-10-1[0123456789]-*.nc')  #first year 10 second days of october for training\n",
    "\n",
    "f_mli = sorted([*f_mli1, *f_mli2]) #the * is to have at the end just a list of file <=> sorted(f_mli1+f_mli2), no idea why they sort\n",
    "\n",
    "random.shuffle(f_mli)  #I don't see the point of shuffle here\n",
    "f_mli = f_mli[::2]\n",
    "\n",
    "# # debugging\n",
    "# f_mli = f_mli[0:72*5]\n",
    "\n",
    "random.shuffle(f_mli)\n",
    "print(f'[TRAIN] Total # of input files: {len(f_mli)}')\n",
    "print(f'[TRAIN] Total # of columns (nfiles * ncols): {len(f_mli)*384}')\n",
    "tds = load_nc_dir_with_generator(f_mli)\n",
    "tds = tds.unbatch()\n",
    "tds = tds.shuffle(buffer_size=shuffle_buffer, reshuffle_each_iteration=True) #lot of shuffle\n",
    "tds = tds.prefetch(buffer_size=4) # in realtion to the batch size    these 3 last line seems useless since we'll do it at each epoch\n",
    "\n",
    "# for validation\n",
    "\n",
    "# # First 5 days of each month for the following 2 years\n",
    "# f_mli1 = glob.glob('/pscratch/sd/s/sungduk/hugging/E3SM-MMF_ne4/train/*/E3SM-MMF.mli.0007-0[23456789]-0[12345]-*.nc')\n",
    "# f_mli2 = glob.glob('/pscratch/sd/s/sungduk/hugging/E3SM-MMF_ne4/train/*/E3SM-MMF.mli.0007-1[012]-0[12345]-*.nc')\n",
    "# f_mli3 = glob.glob('/pscratch/sd/s/sungduk/hugging/E3SM-MMF_ne4/train/*/E3SM-MMF.mli.000[89]-*-0[12345]-*.nc')\n",
    "# f_mli_val = [*f_mli1, *f_mli2, *f_mli3]\n",
    "\n",
    "# every 2th sample\n",
    "f_mli1 = glob.glob(path_data+'/0002/10/E3SM-MMF.mli.0002-10-0[123456789]-*.nc')\n",
    "f_mli2 = glob.glob(path_data+'/0002/10/E3SM-MMF.mli.0002-10-1[012345678]-*.nc')\n",
    "f_mli3 = glob.glob(path_data+'/0002/10/E3SM-MMF.mli.000[2]-10-2[012345678]-*.nc')\n",
    "f_mli_val = sorted([*f_mli1, *f_mli2, *f_mli3])\n",
    "f_mli_val = f_mli_val[::10]\n",
    "\n",
    "# # debugging\n",
    "# f_mli_val = f_mli_val[0:72*5]\n",
    "\n",
    "random.shuffle(f_mli_val)\n",
    "print(f'[VAL] Total # of input files: {len(f_mli_val)}')\n",
    "print(f'[VAL] Total # of columns (nfiles * ncols): {len(f_mli_val)*384}')\n",
    "tds_val = load_nc_dir_with_generator(f_mli_val)\n",
    "tds_val = tds_val.shuffle(buffer_size=shuffle_buffer, reshuffle_each_iteration=True)\n",
    "tds_val = tds_val.prefetch(buffer_size=4) # in realtion to the batch size\n",
    "\n",
    "#list(tds)\n",
    "# for count_batch in tds.repeat().batch(10).take(1):\n",
    "#     print(count_batch[0].numpy())\n",
    "#count_batch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3e7ab13-9b97-44f8-8d69-fbbf0c391538",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3230aef8-2a79-44b3-8558-f830865191a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Emulator\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input (InputLayer)             [(None, 124)]        0           []                               \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 512)          64000       ['input[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 512)          262656      ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 128)          65664       ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 120)          15480       ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 8)            1032        ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 128)          0           ['dense_8[0][0]',                \n",
      "                                                                  'dense_9[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 408,832\n",
      "Trainable params: 408,832\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Model given by ClimSim (not the one they are using in the paper)\n",
    "#No idea what is this one if it is not juste a test example\n",
    "\n",
    "\n",
    "\n",
    "# model params\n",
    "input_length = 2*60 + 4  #temperature and humidity have the vertical dimension\n",
    "output_length_lin  = 2*60  #only temperature and moisting tendency\n",
    "\n",
    "output_length_relu = 8\n",
    "output_length = output_length_lin + output_length_relu\n",
    "n_nodes = 512\n",
    "\n",
    "# constrcut a model\n",
    "input_layer    = keras.layers.Input(shape=(input_length,), name='input')\n",
    "hidden_0       = keras.layers.Dense(n_nodes, activation='relu')(input_layer)\n",
    "hidden_1       = keras.layers.Dense(n_nodes, activation='relu')(hidden_0)\n",
    "output_pre     = keras.layers.Dense(output_length, activation='elu')(hidden_1)\n",
    "output_lin     = keras.layers.Dense(output_length_lin,activation='linear')(output_pre)\n",
    "output_relu    = keras.layers.Dense(output_length_relu,activation='relu')(output_pre)\n",
    "output_layer   = keras.layers.Concatenate()([output_lin, output_relu])\n",
    "\n",
    "model = keras.Model(input_layer, output_layer, name='Emulator')\n",
    "model.summary()\n",
    "\n",
    "# compile\n",
    "model.compile(optimizer=keras.optimizers.Adam(), #optimizer=keras.optimizers.Adam(learning_rate=clr),\n",
    "              loss='mse',\n",
    "              metrics=['mse','mae','accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f39ca77b-4959-4861-80c6-39471fbd35f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# callbacks\n",
    "# a. tensorboard\n",
    "tboard_callback = keras.callbacks.TensorBoard(log_dir = './logs_tensorboard',\n",
    "                                              histogram_freq = 1,)\n",
    "\n",
    "# b. checkpoint\n",
    "filepath_checkpoint = 'saved_model/best_model_proto.h5'\n",
    "checkpoint_callback = keras.callbacks.ModelCheckpoint(filepath=filepath_checkpoint,\n",
    "                                                            save_weights_only=False,\n",
    "                                                            monitor='val_mse',\n",
    "                                                            mode='min',\n",
    "                                                            save_best_only=True)\n",
    "\n",
    "# c. csv logger\n",
    "filepath_csv = 'csv_logger.txt'\n",
    "csv_callback = keras.callbacks.CSVLogger(filepath_csv, separator=\",\", append=True)\n",
    "\n",
    "my_callbacks= [tboard_callback, checkpoint_callback, csv_callback]\n",
    "\n",
    "# !mkdir logs_tensorboard\n",
    "# !mkdir saved_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae19d011-9cc7-4f42-97e0-442653999ba7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "   2736/Unknown - 218s 80ms/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0321 - accuracy: 0.9518"
     ]
    }
   ],
   "source": [
    "# Manually shuffling the order of input files.\n",
    "# \"tds = tds.shuffle(buffer_size=<global>, reshuffle_each_iteration=True)\" is possible,\n",
    "# however, it is slow.\n",
    "# So employing global shuffle (by file names) + local shuffle (using .shuffle).\n",
    "\n",
    "N_EPOCHS = 2\n",
    "shuffle_buffer = 12*384 #ncol=384\n",
    "batch_size= 96 # 384/4\n",
    "\n",
    "n=0\n",
    "while n < N_EPOCHS:\n",
    "    random.shuffle(f_mli)\n",
    "    tds = load_nc_dir_with_generator(f_mli) # global shuffle by file names\n",
    "    tds = tds.unbatch()\n",
    " # local shuffle by elements    tds = tds.shuffle(buffer_size=shuffle_buffer, reshuffle_each_iteration=False)\n",
    "    tds = tds.batch(batch_size)\n",
    "    tds = tds.prefetch(buffer_size=int(shuffle_buffer/384)) # in realtion to the batch size\n",
    "\n",
    "    random.shuffle(f_mli_val)\n",
    "    tds_val = load_nc_dir_with_generator(f_mli_val)\n",
    "    tds_val = tds_val.unbatch()\n",
    "    tds_val = tds_val.shuffle(buffer_size=shuffle_buffer, reshuffle_each_iteration=False)\n",
    "    tds_val = tds_val.batch(batch_size)\n",
    "    tds_val = tds_val.prefetch(buffer_size=int(shuffle_buffer/384))\n",
    "    \n",
    "    print(f'Epoch: {n+1}')\n",
    "    model.fit(tds, validation_data=tds_val, callbacks=my_callbacks)\n",
    "    #model.fit(tds, validation_data=tds_val)\n",
    "    n+=1\n",
    "    \n",
    "model.save('my_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8fd8f2a-9570-400a-9441-27fe59346fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = tf.keras.models.load_model('my_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bae4f88c-1fd7-4915-a1fc-71aca12bc3f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Emulator\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input (InputLayer)             [(None, 124)]        0           []                               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 512)          64000       ['input[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 512)          262656      ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 128)          65664       ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 120)          15480       ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 8)            1032        ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 128)          0           ['dense_3[0][0]',                \n",
      "                                                                  'dense_4[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 408,832\n",
      "Trainable params: 408,832\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0d8c813-2ac4-4fb9-9e4d-270758aa69ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1681920, 124)\n",
      "(1681920, 128)\n"
     ]
    }
   ],
   "source": [
    "fn_x_input = '/gpfsscratch/rech/psl/upu87pm/preprocessed_data/scoring_input.npy'\n",
    "fn_x_target = '/gpfsscratch/rech/psl/upu87pm/preprocessed_data/scoring_target.npy'\n",
    "\n",
    "input_set = x_true = np.load(fn_x_input).astype(np.float64)\n",
    "target_set = x_true = np.load(fn_x_target).astype(np.float64)\n",
    "\n",
    "\n",
    "print(input_set.shape)\n",
    "print(target_set.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35b6586e-79b6-45aa-80c1-b9d8306230c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "mini_set_input = np.array([input_set[i,:] for i in range(1000)])\n",
    "\n",
    "\n",
    "f_input_save =  '../data/npy_data/first_test_input_set.npy'\n",
    "\n",
    "\n",
    "\n",
    "f_save = 'npy_data/prediction/first_test_prediction.npy'\n",
    "np.save(f_input_save, mini_set_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875a2eaa-a529-4f8e-bb12-0bbd652e201b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-2.9.1_py3.10",
   "language": "python",
   "name": "module-conda-env-tensorflow-2.9.1_py3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
