					ClimSim
					
		Overview:
					
lack of resolution due to computational constraints  --->  inacurate and imprecise predictions (1) of critical processes (storms, cloud and extreme rainfall physics)
	--> hybrid method(2) -> cheaper and more accurate
	
(1) parameterization -> empirical mathematical representation of a smaler scale


curently, the scale is around 100 km but for cloud formation we would need ~100m (6 order of magnitude and much more in computational cost)

(2) -dynamic: traditional numerical methods—which solve the equations governing
large-scale fluid motions of Earth’s atmosphere
    -physic: with ML emulators of the macro-scale effects of small-scale physics
   --> learn directly from data generated by short-duration, high-resolution simulations
   

GOAL: an ML parameterization emulator return the large-scale outputs—changes in wind, moisture, or temperature—that occur due to unresolved small-scale (sub-resolution) physics, given large-scale resolved inputs (e.g., temperature, wind velocity)


data generated with multiscale method: deploy a smaller scale, high resolution cloud resolving simulator for each host grid column (CRM). The smaller scale simulator explicitly resolves the detailed behaviour of clouds and their turbulent motions. The time-integrated and horizontally averaged influence of the resolved convection is fed upscale to the host climate simulator, and is the target of hybrid ML-climate simulation approaches.
ClimSim utilize a 2D CRM with 64 columns and 2 km horizontal grid spacing within each grid cell.

--> 60 levels

		Dataset:
		

For the low resolution


         x (di = 124)       ==>       y (do = 128)

Temperature               60 |Heating tendency              60
Humidity                  60 |Moistening tendency           60
Surface pressure           1 |Net surface shortwave flux     1
Insolation                 1 |Downward surface longwave flux 1
Surface latent heat flux   1 |snow rate                      1  
surface sensible heat flux 1 |rain rate                      1
			     |Visible direct solar flux      1
			     |Near-IR direct solar flux      1
			     |Visible diffused solar flux    1
			     |Near-IR diffused solar flux    1

 (Heating tendency + Moistening tendency + Net surface shortwave flux + Downward surface longwave flux + snow rate + rain rate + visible direct solar flux +  

saved value of the atmospheric state  before and after high-resolution caclulation
saved at 20 minutes intervals for 10 years --> 5.7 billion samples (21600*72*3650) for the high resolution that use a unstruncted grid with 21.600 colums (41 TB). (resp. 384 colums for low resolution (744 GB)) and an aquaplanet

one time step -> one input and one output file -> 525 000 files
dimension:24 2D variables = horizontal + time (solar insolation, snow depth, ..)
	  10 3D variables = horizontal + time + vertical (temperature, humidity, wind)
where horizontal is 1D since it is an unstructured grid 

the 10 years are divided (with one month gap):
	(i)   training   | first 7 years
	(ii)  validation | 8th year
	(iii) test       | last 2 years
	

Preprocessing:
	(1) downsample in time using every 7th sample
	(2) collapse horizontal location and time into a single sample dimension
	(3) normalize variables , with these statistics calculated separately at each of the 60 vertical levels for the four variables with vertical dependence
	(4) concatenate variables into multi-variate input and output vectors for each sample




-> each sample has 60 levels (vertical)





######################
## JEANZAY    IDRIS ##
######################
#SBATCH --job-name=test
#SBATCH --output=TEST_Script_Output_ico
#SBATCH --error=TEST_Script_Output_ico
#SBATCH --nodes=1
#SBATCH --ntasks=10
#SBATCH --ntasks-per-node=10
#SBATCH --hint=nomultithread
#SBATCH --time=02:00:00
#SBATCH --qos=qos_cpu-dev
#SBATCH --account psl@cpu






locality to improve ML
